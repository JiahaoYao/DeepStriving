# 	Optimization

### Implement

-   SGD
-   Adam
-   Adamax
-   RMSProp
-   AdaGrad
-   [NAdam](http://cs229.stanford.edu/proj2015/054_report.pdf)

<div align = 'center'>
<img src = "https://github.com/JiahaoYao/DeepStriving/blob/master/examples/opts/images/opt.png" width='600px'>
</div>

### How to Reproduce

You can open the Python Notebook, and run the examples I have provided.

### Reference

-   [Overview of gradient descent](http://ruder.io/optimizing-gradient-descent/)

    â€‹